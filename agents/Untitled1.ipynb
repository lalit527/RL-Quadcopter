{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.99, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.tanh(reward - 0.0005*(abs(self.sim.pose[:3] - self.target_pos).sum() + \n",
    "                         0.0001*(abs(self.sim.v).sum() + abs(self.sim.pose[3:6]).sum() + dis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.tanh(reward - 0.0005*(abs(self.sim.pose[:3] - self.target_pos).sum() + distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Activation at 0xb299295f8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = 0\n",
    "        penalty = 0\n",
    "        current_position = self.sim.pose[:3]\n",
    "        penalty = np.sqrt(np.square(current_position[0]-self.target_pos[0]))\n",
    "        penalty += np.sqrt(np.square(current_position[1]-self.target_pos[1]))\n",
    "        penalty += 10*np.sqrt(np.square(current_position[2]-self.target_pos[2]))\n",
    "        penalty += (abs(self.sim.pose[:3] - self.target_pos).sum() + abs(self.sim.v).sum() + abs(self.sim.pose[3:6]).sum())\n",
    "        \n",
    "        reward = 1\n",
    "\n",
    "        distance = np.sqrt(np.square(current_position[0]-self.target_pos[0]) + np.square(current_position[1]-self.target_pos[1]) + np.square(current_position[2]-self.target_pos[2]))\n",
    "        \n",
    "        if distance < 10:\n",
    "            reward += 10\n",
    "        \n",
    "        reward = np.tanh(reward - 0.0005*penalty)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.tanh(1 - 0.0005*(abs(self.sim.pose[:3] - self.target_pos).sum() + \n",
    "                             abs(self.sim.v).sum() + abs(self.sim.pose[3:6]).sum() + penalty))\n",
    "\n",
    "\n",
    "\n",
    "penalty = np.sqrt(np.square(current_position[0]-self.target_pos[0]))\n",
    "penalty += np.sqrt(np.square(current_position[1]-self.target_pos[1]))\n",
    "penalty += 2*np.sqrt(np.square(current_position[2]-self.target_pos[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = 0\n",
    "        penalty = 0\n",
    "        current_position = self.sim.pose[:3]\n",
    "        penalty += abs(self.sim.pose[3:6]).sum()\n",
    "        penalty += abs(abs(current_position-self.target_pos).sum() - abs(self.sim.v).sum())\n",
    "        \n",
    "        penalty += np.square(current_position[0]-self.target_pos[0])\n",
    "        penalty += np.square(current_position[1]-self.target_pos[1])\n",
    "        penalty += 2*np.square(current_position[2]-self.target_pos[2])\n",
    "        \n",
    "        reward = np.tanh(1 - 0.0005*(abs(self.sim.pose[:3] - self.target_pos)).sum())\n",
    "        distance = np.sqrt(np.square(current_position[0]-self.target_pos[0]) + np.square(current_position[1]-self.target_pos[1]) + np.square(current_position[2]-self.target_pos[2]))\n",
    "        if distance < 10:\n",
    "            reward += 10\n",
    "#         # constant reward for flying\n",
    "        reward += 1\n",
    "        return reward - penalty*0.0001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "class Actor:\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, action_low, action_high):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            action_low (array): Min value of each action dimension\n",
    "            action_high (array): Max value of each action dimension\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_low = action_low\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        # Define input layer (states)\n",
    "        states = layers.Input(shape(self.state_size,), name='states')\n",
    "        \n",
    "        net = layers.Dense(units=32, kernel_initializer='glorot_normal')(states)\n",
    "        net = layers.BatchNormalization()(net)\n",
    "        net = layers.Activation('relu')(net)\n",
    "        net = layers.Dropout(0.2)(net)\n",
    "        \n",
    "        net = layers.Dense(units=64)(net)\n",
    "        net = layers.BatchNormalization()(net)\n",
    "        net = layers.Activation('relu')(net)\n",
    "        net = layers.Dropout(0.2)(net)\n",
    "        \n",
    "        net = layers.Dense(units=128)(net)\n",
    "        net = layers.BatchNormalization()(net)\n",
    "        net = layers.Activation('relu')(net)\n",
    "        net = layers.Dropout(0.2)(net)\n",
    "        \n",
    "        raw_actions = layers.Dense(units=self.action_size, activation='sigmoid',\n",
    "            name='raw_actions')(net)\n",
    "        \n",
    "        actions = layers.Lambda(lambda x: (x * self.action_range) + self.action_low,\n",
    "            name='actions')(raw_actions)\n",
    "        \n",
    "        self.model =  models.Model(inputs=states, outputs=actions)\n",
    "        \n",
    "        # Define loss function using action value (Q value) gradients\n",
    "        action_gradients = layers.Input(shape=(self.action_size,))\n",
    "        loss = K.mean(-action_gradients * actions)\n",
    "        \n",
    "        optimizer = optimizers.Adam()\n",
    "        updates_op = optimizer.get_updates(params=self.model.trainable_weight, loss=loss)\n",
    "        self.train_fn = K.function(\n",
    "            inputs=[self.model.input, action_gradients, K.learning_phase()],\n",
    "            outputs=[],\n",
    "            updates=updates_op)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "class Critic:\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        \n",
    "        states = layers.Input(shape=(self.state_size,), name='states')\n",
    "        actions = layers.Input(shape=(self.action_size,), name='actions')\n",
    "        \n",
    "        net_states = layers.Dense(units=32, kernel_initializer='glorot_normal')(states)\n",
    "        net_states = layers.BatchNormalization()(net_states)\n",
    "        net_states = layers.Activation('relu')(net_states)\n",
    "        net_states = layers.Dropout(0.2)(net_states)\n",
    "        \n",
    "        net_states = layers.Dense(units=64)(net_states)\n",
    "        net_states = layers.BatchNormalization()(net_states)\n",
    "        net_states = layers.Activation('relu')(net_states)\n",
    "        net_states = layers.Dropout(0.2)(net_states)\n",
    "        \n",
    "        net_states = layers.Dense(units=128)(net_states)\n",
    "        net_states = layers.BatchNormalization()(net_states)\n",
    "        net_states = layers.Activation('relu')(net_states)\n",
    "        net_states = layers.Dropout(0.2)(net_states)\n",
    "        \n",
    "        net_actions = layers.Dense(units=32, kernel_initializer='glorot_normal')(actions)\n",
    "        net_actions = layers.BatchNormalization()(net_actions)\n",
    "        net_actions = layers.Activation('relu')(net_actions)\n",
    "        net_actions = layers.Dropout(0.2)(net_actions)\n",
    "        \n",
    "        net_actions = layers.Dense(units=64)(net_actions)\n",
    "        net_actions = layers.BatchNormalization()(net_actions)\n",
    "        net_actions = layers.Activation('relu')(net_actions)\n",
    "        net_actions = layers.Dropout(0.2)(net_actions)\n",
    "        \n",
    "        net_actions = layers.Dense(units=128)(net_actions)\n",
    "        net_actions = layers.BatchNormalization()(net_actions)\n",
    "        net_actions = layers.Activation('relu')(net_actions)\n",
    "        net_actions = layers.Dropout(0.2)(net_actions)\n",
    "        \n",
    "        \n",
    "        net = layers.Add()([net_states, net_actions])\n",
    "        net = layers.Activation('relu')(net)\n",
    "        \n",
    "        Q_values = layers.Dense(units=1, name='q_values')(net)\n",
    "        \n",
    "        self.model = models.Model(inputs=[states, actions], outputs=Q_values)\n",
    "        \n",
    "        optimizer = optimizer.Adam()\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "        \n",
    "        action_gradients = K.gradients(Q_values, actions)\n",
    "        \n",
    "        self.get_action_gradients = K.function(inputs=[*self.model.input, K.learning_phase()],\n",
    "            outputs=action_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from agents.actor import Actor\n",
    "from agents.critic import Critic\n",
    "from agents.replaybuffer import ReplayBuffer\n",
    "from agents.ounoise import OUNoise\n",
    "\n",
    "class DDPG:\n",
    "    \"\"\"Reinforcement Learning agent that learns using DDPG.\"\"\"\n",
    "    def __init__(self, task):\n",
    "        self.task = task.state_size\n",
    "        self.action_size = task.action_size\n",
    "        self.action_low = task.action_low\n",
    "        self.action_high = task.action_high\n",
    "        \n",
    "        self.actor_local = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n",
    "        \n",
    "        self.critic_local = Critic(self.size, self.action_size)\n",
    "        self.critic_target = Critic(Self.size, self.action_size)\n",
    "        \n",
    "        self.critic_target.model.set_weights(self.critic_local.model.get_weights())\n",
    "        self.actor_target.model.set_weights(self.actor_local.model.get_weights())\n",
    "        \n",
    "        self.exploration_mu = 0\n",
    "        self.exploration_theta = 0.15\n",
    "        self.exploration_sigma = 0.2\n",
    "        self.noise = OUNoise(self.action_size, self.exploration_mu, self.exploration_theta, self.exploration_sigma)\n",
    "\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 64\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "        \n",
    "        self.gamma = 0.09\n",
    "        self.tau = 0.01\n",
    "        \n",
    "    def reset_episode(self):\n",
    "        self.noise.reset()\n",
    "        state = self.task.reset()\n",
    "        self.last_state = state\n",
    "        return state\n",
    "    \n",
    "    def step(self, action, reward, next_state, done):\n",
    "        self.memory.add(self.last_state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "            \n",
    "        self.last_state = next_state\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state(s) as per current policy.\"\"\"\n",
    "        state = np.reshape(state, [-1, self.state_size])\n",
    "        action = self.actor_local.model.predict(state)[0]\n",
    "        return list(action + self.noise.sample()) \n",
    "    \n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\"\"\"\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.array([e.action for e in experiences if e is not None]).astype(np.float32).reshape(-1, self.action_size)\n",
    "        rewards = np.array([e.reward for e in experiences if e is not None]).astype(np.float32).reshape(-1, 1)\n",
    "        dones = np.array([e.done for e in experiences if e is not None]).astype(np.uint8).reshape(-1, 1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "        \n",
    "        actions_next = self.actor_target.model.predict_on_batch(next_states)\n",
    "        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])\n",
    "        \n",
    "        Q_targets = rewards + self.gamma + Q_targets_next * (1 - dones)\n",
    "        self.critic_local,model.train_on_batch(x=[states, actions], y=Q_targets)\n",
    "        \n",
    "        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]), (-1, self.action_size))\n",
    "        self.actor_local.train_fn([states, action_gradients, 1])\n",
    "        \n",
    "        self.soft_update(self.critic_local.model, self.critic_target.model)\n",
    "        self.soft_update(self.actor_local.model, self.actor_target.model)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        local_weights = np.array(local_model.get_weights())\n",
    "        target_weights = np.array(target_model.get_weights())\n",
    "        \n",
    "        assert len(local_weights) == len(target_weights), \"Local and target model parameters must have the same size\"\n",
    "\n",
    "        new_weights = self.tau * local_weights + (1 - self.tau) * target_weights\n",
    "        target_model.set_weights(new_weights)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lality/projects/personal/Udacity/MLND/RL-Quadcopter-2/agents\r\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size: maximum size of buffer\n",
    "            batch_size: size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self, batch_size=64):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        return random.sample(self.memory, k=self.batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "    def __init__(self, size, mu, theta, sigma):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
